{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages"
      ],
      "metadata": {
        "id": "3DQjq22Ea-_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQLp3PLZa6iX",
        "outputId": "393bb927-7def-4cee-e825-e1c336050554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = [\n",
        "    'gradio',\n",
        "    'PyPDF2',\n",
        "    'python-docx',\n",
        "    'openpyxl',\n",
        "    'pytesseract',\n",
        "    'Pillow',\n",
        "    'transformers',\n",
        "    'torch',\n",
        "    'nltk',\n",
        "    'spacy',\n",
        "    'textstat',\n",
        "    'langdetect',\n",
        "    'pyngrok'\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install additional system dependencies for OCR\n",
        "subprocess.check_call([\"apt-get\", \"update\"])\n",
        "subprocess.check_call([\"apt-get\", \"install\", \"-y\", \"tesseract-ocr\"])\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all necessary libraries"
      ],
      "metadata": {
        "id": "GuwC0AdLbGdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import PyPDF2\n",
        "import docx\n",
        "import openpyxl\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "import spacy\n",
        "import textstat\n",
        "from langdetect import detect\n",
        "import hashlib\n",
        "import mimetypes\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "GmPD_o9kbKi1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download required NLTK data"
      ],
      "metadata": {
        "id": "puXVexygbQ9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnB-Svi5bUEv",
        "outputId": "75aed25e-3262-412b-b432-ef6cc55d2815"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load spaCy model"
      ],
      "metadata": {
        "id": "CDUF1ARRbX-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading spaCy model...\")\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 5: Text extraction functions for different file formats\n",
        "\n",
        "# PDF text extraction\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Extract text content from PDF files\"\"\"\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting PDF: {str(e)}\"\n",
        "\n",
        "# DOCX text extraction\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"Extract text content from DOCX files\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        text = \"\"\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting DOCX: {str(e)}\"\n",
        "\n",
        "# Excel text extraction\n",
        "def extract_text_from_excel(file_path):\n",
        "    \"\"\"Extract text content from Excel files\"\"\"\n",
        "    try:\n",
        "        workbook = openpyxl.load_workbook(file_path)\n",
        "        text = \"\"\n",
        "        for sheet_name in workbook.sheetnames:\n",
        "            sheet = workbook[sheet_name]\n",
        "            text += f\"Sheet: {sheet_name}\\n\"\n",
        "            for row in sheet.iter_rows(values_only=True):\n",
        "                row_text = \" \".join([str(cell) if cell is not None else \"\" for cell in row])\n",
        "                if row_text.strip():\n",
        "                    text += row_text + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting Excel: {str(e)}\"\n",
        "\n",
        "# Image OCR text extraction\n",
        "def extract_text_from_image(file_path):\n",
        "    \"\"\"Extract text from images using OCR\"\"\"\n",
        "    try:\n",
        "        image = Image.open(file_path)\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting image text: {str(e)}\"\n",
        "\n",
        "# TXT file extraction\n",
        "def extract_text_from_txt(file_path):\n",
        "    \"\"\"Extract text from plain text files\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read().strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting TXT: {str(e)}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waBkBUW_bbCw",
        "outputId": "1c872871-c981-4beb-ac27-a653696e6f38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading spaCy model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main text extraction function"
      ],
      "metadata": {
        "id": "q1lPl83_bh3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"Extract text from various file formats\"\"\"\n",
        "    file_extension = Path(file_path).suffix.lower()\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    elif file_extension in ['.xlsx', '.xls']:\n",
        "        return extract_text_from_excel(file_path)\n",
        "    elif file_extension in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']:\n",
        "        return extract_text_from_image(file_path)\n",
        "    elif file_extension == '.txt':\n",
        "        return extract_text_from_txt(file_path)\n",
        "    else:\n",
        "        return \"Unsupported file format\""
      ],
      "metadata": {
        "id": "0jb_8VHubk-t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic content identification functions"
      ],
      "metadata": {
        "id": "lHVkZm80boXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_key_entities(text):\n",
        "    \"\"\"Identify named entities in the text\"\"\"\n",
        "    doc = nlp(text[:1000000])  # Limit text length for processing\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append({\n",
        "            'text': ent.text,\n",
        "            'label': ent.label_,\n",
        "            'description': spacy.explain(ent.label_)\n",
        "        })\n",
        "    return entities\n",
        "\n",
        "def extract_keywords(text, num_keywords=10):\n",
        "    \"\"\"Extract important keywords from text\"\"\"\n",
        "    doc = nlp(text[:1000000])\n",
        "\n",
        "    # Filter tokens (remove stop words, punctuation, spaces)\n",
        "    tokens = [token.lemma_.lower() for token in doc\n",
        "              if not token.is_stop and not token.is_punct and not token.is_space\n",
        "              and len(token.text) > 2]\n",
        "\n",
        "    # Count frequency\n",
        "    word_freq = Counter(tokens)\n",
        "    return word_freq.most_common(num_keywords)\n",
        "\n",
        "def analyze_document_structure(text):\n",
        "    \"\"\"Analyze document structure and content\"\"\"\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    structure_info = {\n",
        "        'total_lines': len(lines),\n",
        "        'non_empty_lines': len([line for line in lines if line.strip()]),\n",
        "        'average_line_length': sum(len(line) for line in lines) / len(lines) if lines else 0,\n",
        "        'has_headers': any(line.isupper() and len(line.split()) <= 5 for line in lines[:20]),\n",
        "        'has_numbered_sections': any(re.match(r'^\\d+\\.', line.strip()) for line in lines),\n",
        "        'has_bullet_points': any(line.strip().startswith(('•', '-', '*')) for line in lines)\n",
        "    }\n",
        "\n",
        "    return structure_info"
      ],
      "metadata": {
        "id": "sQAlbLobbrWV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Content analysis functions"
      ],
      "metadata": {
        "id": "Op6QUdmMb0K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_readability(text):\n",
        "    \"\"\"Analyze text readability metrics\"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "\n",
        "    readability_scores = {\n",
        "        'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
        "        'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),\n",
        "        'automated_readability_index': textstat.automated_readability_index(text),\n",
        "        'coleman_liau_index': textstat.coleman_liau_index(text),\n",
        "        'reading_time_minutes': textstat.reading_time(text, ms_per_char=14.69)\n",
        "    }\n",
        "\n",
        "    return readability_scores\n",
        "\n",
        "def detect_language(text):\n",
        "    \"\"\"Detect the language of the text\"\"\"\n",
        "    try:\n",
        "        if text.strip():\n",
        "            return detect(text[:1000])  # Use first 1000 chars for detection\n",
        "        return \"unknown\"\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def get_text_statistics(text):\n",
        "    \"\"\"Get basic text statistics\"\"\"\n",
        "    words = text.split()\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    stats = {\n",
        "        'character_count': len(text),\n",
        "        'character_count_no_spaces': len(text.replace(' ', '')),\n",
        "        'word_count': len(words),\n",
        "        'sentence_count': len(sentences),\n",
        "        'paragraph_count': len([p for p in text.split('\\n\\n') if p.strip()]),\n",
        "        'average_words_per_sentence': len(words) / len(sentences) if sentences else 0,\n",
        "        'average_characters_per_word': sum(len(word) for word in words) / len(words) if words else 0\n",
        "    }\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "48uDW8K-b4uW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File information extraction"
      ],
      "metadata": {
        "id": "JBwCXBBSb-vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_info(file_path):\n",
        "    \"\"\"Get basic file information\"\"\"\n",
        "    try:\n",
        "        file_stat = os.stat(file_path)\n",
        "        file_info = {\n",
        "            'filename': os.path.basename(file_path),\n",
        "            'file_extension': Path(file_path).suffix.lower(),\n",
        "            'file_size_bytes': file_stat.st_size,\n",
        "            'file_size_mb': round(file_stat.st_size / (1024 * 1024), 2),\n",
        "            'created_date': datetime.datetime.fromtimestamp(file_stat.st_ctime).isoformat(),\n",
        "            'modified_date': datetime.datetime.fromtimestamp(file_stat.st_mtime).isoformat(),\n",
        "            'mime_type': mimetypes.guess_type(file_path)[0]\n",
        "        }\n",
        "\n",
        "        # Generate file hash for uniqueness\n",
        "        with open(file_path, 'rb') as f:\n",
        "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
        "        file_info['file_hash'] = file_hash\n",
        "\n",
        "        return file_info\n",
        "    except Exception as e:\n",
        "        return {'error': f\"Error getting file info: {str(e)}\"}"
      ],
      "metadata": {
        "id": "mYEPmcSEcEO2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main metadata generation function"
      ],
      "metadata": {
        "id": "xTrcFwjicMSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_metadata(file_path, text_content):\n",
        "    \"\"\"Generate comprehensive metadata for a document\"\"\"\n",
        "\n",
        "    print(\"Generating metadata...\")\n",
        "\n",
        "    # Basic file information\n",
        "    file_info = get_file_info(file_path)\n",
        "\n",
        "    # Text statistics\n",
        "    text_stats = get_text_statistics(text_content)\n",
        "\n",
        "    # Language detection\n",
        "    language = detect_language(text_content)\n",
        "\n",
        "    # Readability analysis\n",
        "    readability = analyze_readability(text_content)\n",
        "\n",
        "    # Document structure analysis\n",
        "    structure = analyze_document_structure(text_content)\n",
        "\n",
        "    # Keywords extraction\n",
        "    keywords = extract_keywords(text_content)\n",
        "\n",
        "    # Named entities\n",
        "    entities = identify_key_entities(text_content)\n",
        "\n",
        "    # Compile all metadata\n",
        "    metadata = {\n",
        "        'generation_timestamp': datetime.datetime.now().isoformat(),\n",
        "        'file_information': file_info,\n",
        "        'content_statistics': text_stats,\n",
        "        'language': language,\n",
        "        'readability_scores': readability,\n",
        "        'document_structure': structure,\n",
        "        'keywords': [{'word': word, 'frequency': freq} for word, freq in keywords],\n",
        "        'named_entities': entities[:20],  # Limit to top 20 entities\n",
        "        'content_preview': text_content[:500] + \"...\" if len(text_content) > 500 else text_content,\n",
        "        'metadata_version': '1.0'\n",
        "    }\n",
        "\n",
        "    return metadata\n"
      ],
      "metadata": {
        "id": "Y42bjtiXcNbd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio interface functions"
      ],
      "metadata": {
        "id": "t-Ioc_vgcT2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file):\n",
        "    \"\"\"Process uploaded file and generate metadata\"\"\"\n",
        "    if file is None:\n",
        "        return \"Please upload a file first.\", \"{}\"\n",
        "\n",
        "    try:\n",
        "        # Extract text from file\n",
        "        text_content = extract_text_from_file(file.name)\n",
        "\n",
        "        if text_content.startswith(\"Error\") or text_content == \"Unsupported file format\":\n",
        "            return text_content, \"{}\"\n",
        "\n",
        "        # Generate metadata\n",
        "        metadata = generate_metadata(file.name, text_content)\n",
        "\n",
        "        # Format metadata as JSON string for display\n",
        "        metadata_json = json.dumps(metadata, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Create summary for display\n",
        "        summary = f\"\"\"\n",
        "        📄 **File Analysis Complete!**\n",
        "\n",
        "        **File:** {metadata['file_information'].get('filename', 'Unknown')}\n",
        "        **Size:** {metadata['file_information'].get('file_size_mb', 0)} MB\n",
        "        **Language:** {metadata.get('language', 'Unknown')}\n",
        "        **Words:** {metadata['content_statistics'].get('word_count', 0):,}\n",
        "        **Characters:** {metadata['content_statistics'].get('character_count', 0):,}\n",
        "        **Reading Time:** {metadata['readability_scores'].get('reading_time_minutes', 0):.1f} minutes\n",
        "\n",
        "        **Top Keywords:**\n",
        "        {chr(10).join([f\"• {kw['word']} ({kw['frequency']})\" for kw in metadata['keywords'][:5]])}\n",
        "\n",
        "        **Content Preview:**\n",
        "        {metadata['content_preview']}\n",
        "        \"\"\"\n",
        "\n",
        "        return summary, metadata_json\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing file: {str(e)}\", \"{}\"\n",
        "\n",
        "def download_metadata(metadata_json):\n",
        "    \"\"\"Create downloadable metadata file\"\"\"\n",
        "    if not metadata_json or metadata_json == \"{}\":\n",
        "        return None\n",
        "\n",
        "    # Save metadata to temporary file\n",
        "    temp_file = \"/tmp/metadata.json\"\n",
        "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(metadata_json)\n",
        "\n",
        "    return temp_file"
      ],
      "metadata": {
        "id": "JSOiOydDcXRX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Gradio interface"
      ],
      "metadata": {
        "id": "bHATUhlpchld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating web interface...\")\n",
        "\n",
        "# Custom CSS for better styling\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "}\n",
        ".main-header {\n",
        "    text-align: center;\n",
        "    color: #2c3e50;\n",
        "    margin-bottom: 2rem;\n",
        "}\n",
        ".upload-area {\n",
        "    border: 2px dashed #3498db;\n",
        "    border-radius: 10px;\n",
        "    padding: 2rem;\n",
        "    text-align: center;\n",
        "    background-color: #f8f9fa;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create the interface\n",
        "with gr.Blocks(css=custom_css, title=\"Automated Metadata Generation System\") as interface:\n",
        "\n",
        "    # Header\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🤖 Automated Metadata Generation System\n",
        "\n",
        "    Upload any document (PDF, DOCX, Excel, Images, TXT) and get comprehensive metadata analysis including:\n",
        "    - **Content Statistics** (word count, readability scores)\n",
        "    - **Semantic Analysis** (keywords, entities)\n",
        "    - **Document Structure** analysis\n",
        "    - **Language Detection**\n",
        "    - **File Information** and more!\n",
        "    \"\"\", elem_classes=[\"main-header\"])\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # File upload\n",
        "            file_input = gr.File(\n",
        "                label=\"📁 Upload Document\",\n",
        "                file_types=[\".pdf\", \".docx\", \".xlsx\", \".xls\", \".txt\", \".png\", \".jpg\", \".jpeg\"],\n",
        "                elem_classes=[\"upload-area\"]\n",
        "            )\n",
        "\n",
        "            # Process button\n",
        "            process_btn = gr.Button(\"🔍 Generate Metadata\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Results display\n",
        "            result_display = gr.Markdown(label=\"📊 Analysis Results\")\n",
        "\n",
        "    # Metadata JSON output (hidden by default)\n",
        "    with gr.Accordion(\"🔧 Raw Metadata (JSON)\", open=False):\n",
        "        metadata_output = gr.Textbox(\n",
        "            label=\"Complete Metadata\",\n",
        "            lines=20,\n",
        "            max_lines=30,\n",
        "            show_copy_button=True\n",
        "        )\n",
        "\n",
        "    # Download section\n",
        "    with gr.Row():\n",
        "        download_btn = gr.File(label=\"💾 Download Metadata JSON\", visible=True)\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn.click(\n",
        "        process_file,\n",
        "        inputs=[file_input],\n",
        "        outputs=[result_display, metadata_output]\n",
        "    )\n",
        "\n",
        "    # Auto-generate download file when metadata is updated\n",
        "    metadata_output.change(\n",
        "        download_metadata,\n",
        "        inputs=[metadata_output],\n",
        "        outputs=[download_btn]\n",
        "    )\n",
        "\n",
        "    # Example section\n",
        "    gr.Markdown(\"\"\"\n",
        "    ## 📝 Supported File Types:\n",
        "    - **PDF** documents\n",
        "    - **Microsoft Word** (.docx)\n",
        "    - **Excel** spreadsheets (.xlsx, .xls)\n",
        "    - **Text** files (.txt)\n",
        "    - **Images** with text (.png, .jpg, .jpeg) - OCR enabled\n",
        "\n",
        "    ## 🎯 Features:\n",
        "    - ✅ Automatic text extraction\n",
        "    - ✅ Language detection\n",
        "    - ✅ Readability analysis\n",
        "    - ✅ Keyword extraction\n",
        "    - ✅ Named entity recognition\n",
        "    - ✅ Document structure analysis\n",
        "    - ✅ Downloadable metadata\n",
        "    \"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol0wNMrZcoYp",
        "outputId": "0ee6e05d-3729-47a9-b16e-200b75489cdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating web interface...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Launch the interface"
      ],
      "metadata": {
        "id": "FDA-1pJScs1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Launching the web interface...\")\n",
        "print(\"The interface will be available at the URL shown below.\")\n",
        "print(\"You can upload documents and generate metadata automatically!\")\n",
        "\n",
        "# Launch with public sharing enabled\n",
        "interface.launch(\n",
        "    share=True,  # This creates a public URL\n",
        "    server_name=\"0.0.0.0\",\n",
        "    server_port=7860,\n",
        "    show_error=True,\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "print(\"System is now running! Upload a document to generate metadata.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "1xnvm1vNctlk",
        "outputId": "5780ef74-2431-42d8-b083-cab738d0ad34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching the web interface...\n",
            "The interface will be available at the URL shown below.\n",
            "You can upload documents and generate metadata automatically!\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9419117d641efa9fa5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9419117d641efa9fa5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating metadata...\n",
            "Generating metadata...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://9419117d641efa9fa5.gradio.live\n",
            "System is now running! Upload a document to generate metadata.\n"
          ]
        }
      ]
    }
  ]
}